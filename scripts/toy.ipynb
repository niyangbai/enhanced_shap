{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Custom SHAP Approximator with Original SHAP on Simulated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will:\n",
    "\n",
    "- Simulate both tabular and sequential data with known true feature importances.\n",
    "- Train models (XGBoost for tabular data and LSTM for sequential data) on the simulated data.\n",
    "- Compute SHAP values using both the original SHAP package and a custom SHAP approximator.\n",
    "- Compare the SHAP values from both methods with each other and with the true feature importances.\n",
    "- Analyze and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from ESHAP.simulation import TabularDataSimulator, SequentialDataSimulator\n",
    "from ESHAP.shap_engine import TabularSHAPApproximator, SequentialSHAPApproximator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulate Tabular Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate tabular data\n",
    "tabular_simulator = TabularDataSimulator(n_samples=1000, n_features=10)\n",
    "X, y = tabular_simulator.get_data()\n",
    "true_importances = tabular_simulator.get_true_importances()\n",
    "\n",
    "# Convert to NumPy array if necessary\n",
    "if isinstance(X, pd.DataFrame):\n",
    "    X = X.values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "model_xgb = XGBRegressor()\n",
    "model_xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values using SHAP package\n",
    "explainer_shap = shap.TreeExplainer(model_xgb)\n",
    "shap_values_shap = explainer_shap.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize custom SHAP approximator\n",
    "custom_shap_approximator = TabularSHAPApproximator(model_xgb, X_train, num_samples=100)\n",
    "\n",
    "# Compute SHAP values using custom approximator\n",
    "shap_values_custom = custom_shap_approximator.compute_shap_values(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean absolute SHAP values\n",
    "mean_abs_shap_values = np.mean(np.abs(shap_values_shap), axis=0)\n",
    "mean_abs_custom_shap_values = np.mean(np.abs(shap_values_custom), axis=0)\n",
    "\n",
    "# Normalize SHAP values\n",
    "norm_mean_abs_shap_values = mean_abs_shap_values / np.sum(mean_abs_shap_values)\n",
    "norm_mean_abs_custom_shap_values = mean_abs_custom_shap_values / np.sum(mean_abs_custom_shap_values)\n",
    "\n",
    "# Create DataFrame for comparison\n",
    "feature_names = [f'Feature {i}' for i in range(X.shape[1])]\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'True Importance': true_importances,\n",
    "    'Original SHAP': norm_mean_abs_shap_values,\n",
    "    'Custom SHAP': norm_mean_abs_custom_shap_values\n",
    "})\n",
    "\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(feature_names))\n",
    "\n",
    "plt.bar(index, comparison_df['True Importance'], bar_width, label='True Importance')\n",
    "plt.bar(index + bar_width, comparison_df['Original SHAP'], bar_width, label='Original SHAP')\n",
    "plt.bar(index + 2*bar_width, comparison_df['Custom SHAP'], bar_width, label='Custom SHAP')\n",
    "\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Normalized Importance')\n",
    "plt.title('Comparison of True Feature Importance and SHAP Values (XGBoost)')\n",
    "plt.xticks(index + bar_width, feature_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with true importances\n",
    "corr_true_original, _ = pearsonr(true_importances, norm_mean_abs_shap_values)\n",
    "corr_true_custom, _ = pearsonr(true_importances, norm_mean_abs_custom_shap_values)\n",
    "\n",
    "print(f'Correlation between True Importances and Original SHAP (XGBoost): {corr_true_original:.4f}')\n",
    "print(f'Correlation between True Importances and Custom SHAP (XGBoost): {corr_true_custom:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulate Sequential Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate sequential data\n",
    "sequential_simulator = SequentialDataSimulator(n_samples=1000, timesteps=10, features=1)\n",
    "X_seq, y_seq = sequential_simulator.get_data()\n",
    "true_influences = sequential_simulator.get_true_importances()\n",
    "\n",
    "# Ensure correct shape\n",
    "if len(X_seq.shape) != 3:\n",
    "    X_seq = X_seq.reshape((X_seq.shape[0], sequential_simulator.timesteps, sequential_simulator.features))\n",
    "\n",
    "# Split data\n",
    "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(X_seq, y_seq, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model in PyTorch\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Set parameters\n",
    "input_size = sequential_simulator.features\n",
    "hidden_size = 50\n",
    "output_size = 1\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model_lstm = LSTMModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_seq_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "y_train_seq_tensor = torch.tensor(y_train_seq, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Train LSTM model\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_lstm.train()\n",
    "    permutation = torch.randperm(X_train_seq_tensor.size(0))\n",
    "    for i in range(0, X_train_seq_tensor.size(0), batch_size):\n",
    "        indices = permutation[i:i + batch_size]\n",
    "        batch_X, batch_y = X_train_seq_tensor[indices], y_train_seq_tensor[indices]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_lstm(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for SHAP computations\n",
    "X_train_sample_seq = X_train_seq[:100]\n",
    "X_test_sample_seq = X_test_seq[:10]\n",
    "\n",
    "# Compute SHAP values using DeepExplainer\n",
    "model_lstm.eval()\n",
    "X_train_sample_seq_tensor = torch.tensor(X_train_sample_seq, dtype=torch.float32)\n",
    "explainer_shap_seq = shap.DeepExplainer(model_lstm, X_train_sample_seq_tensor)\n",
    "shap_values_shap_seq = explainer_shap_seq.shap_values(torch.tensor(X_test_sample_seq, dtype=torch.float32), check_additivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize custom SHAP approximator\n",
    "custom_shap_approximator_seq = SequentialSHAPApproximator(model_lstm, X_train_sample_seq, num_samples=100)\n",
    "\n",
    "# Compute SHAP values using custom approximator\n",
    "shap_values_custom_seq = custom_shap_approximator_seq.compute_shap_values(X_test_sample_seq)\n",
    "\n",
    "# Compute SHAP values using custom approximator in batch mode\n",
    "shap_values_custom_seq_batch = custom_shap_approximator_seq.batch_approximate_shap_values(X_test_sample_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean absolute SHAP values for the original SHAP values\n",
    "mean_abs_shap_values_seq = np.mean(np.abs(shap_values_shap_seq), axis=0).flatten()\n",
    "# Compute mean absolute SHAP values for the custom SHAP values (per sample and batch mode)\n",
    "mean_abs_custom_shap_values_seq = np.mean(np.abs(shap_values_custom_seq), axis=0).flatten()\n",
    "mean_abs_custom_shap_values_seq_batch = np.mean(np.abs(shap_values_custom_seq_batch), axis=0).flatten()\n",
    "\n",
    "# Normalize SHAP values\n",
    "norm_mean_abs_shap_values_seq = mean_abs_shap_values_seq / np.sum(mean_abs_shap_values_seq)\n",
    "norm_mean_abs_custom_shap_values_seq = mean_abs_custom_shap_values_seq / np.sum(mean_abs_custom_shap_values_seq)\n",
    "norm_mean_abs_custom_shap_values_seq_batch = mean_abs_custom_shap_values_seq_batch / np.sum(mean_abs_custom_shap_values_seq_batch)\n",
    "\n",
    "# Create DataFrame for comparison\n",
    "time_steps = np.arange(len(true_influences))\n",
    "comparison_seq_df = pd.DataFrame({\n",
    "    'Time Step': time_steps,\n",
    "    'True Influence': true_influences,\n",
    "    'Original SHAP': norm_mean_abs_shap_values_seq,\n",
    "    'Custom SHAP (Per Sample)': norm_mean_abs_custom_shap_values_seq,\n",
    "    'Custom SHAP (Batch)': norm_mean_abs_custom_shap_values_seq_batch\n",
    "})\n",
    "\n",
    "print(comparison_seq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison over time steps\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(comparison_seq_df['Time Step'], comparison_seq_df['True Influence'], marker='o', label='True Influence')\n",
    "plt.plot(comparison_seq_df['Time Step'], comparison_seq_df['Original SHAP'], marker='x', label='Original SHAP')\n",
    "plt.plot(comparison_seq_df['Time Step'], comparison_seq_df['Custom SHAP (Per Sample)'], marker='s', label='Custom SHAP (Per Sample)')\n",
    "plt.plot(comparison_seq_df['Time Step'], comparison_seq_df['Custom SHAP (Batch)'], marker='^', label='Custom SHAP (Batch)')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Normalized Importance')\n",
    "plt.title('Comparison of True Influences and SHAP Values Over Time Steps (LSTM)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with true influences\n",
    "corr_true_original_seq, _ = pearsonr(true_influences, norm_mean_abs_shap_values_seq)\n",
    "corr_true_custom_seq, _ = pearsonr(true_influences, norm_mean_abs_custom_shap_values_seq)\n",
    "corr_true_custom_seq_batch, _ = pearsonr(true_influences, norm_mean_abs_custom_shap_values_seq_batch)\n",
    "\n",
    "print(f'Correlation between True Influences and Original SHAP (LSTM): {corr_true_original_seq:.4f}')\n",
    "print(f'Correlation between True Influences and Custom SHAP (Per Sample, LSTM): {corr_true_custom_seq:.4f}')\n",
    "print(f'Correlation between True Influences and Custom SHAP (Batch, LSTM): {corr_true_custom_seq_batch:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulate Sequential Data and Train Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transformer model in PyTorch\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, nhead, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        transformer_out = self.transformer(x)\n",
    "        out = self.fc(transformer_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Set parameters\n",
    "hidden_size = 50\n",
    "nhead = 5\n",
    "num_layers = 2\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model_transformer = TransformerModel(input_size, hidden_size, output_size, nhead, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_transformer.parameters(), lr=0.001)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_seq_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "y_train_seq_tensor = torch.tensor(y_train_seq, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Train Transformer model\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_transformer.train()\n",
    "    permutation = torch.randperm(X_train_seq_tensor.size(0))\n",
    "    for i in range(0, X_train_seq_tensor.size(0), batch_size):\n",
    "        indices = permutation[i:i + batch_size]\n",
    "        batch_X, batch_y = X_train_seq_tensor[indices], y_train_seq_tensor[indices]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_transformer(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for SHAP computations\n",
    "X_train_sample_seq = X_train_seq[:100]\n",
    "X_test_sample_seq = X_test_seq[:10]\n",
    "\n",
    "# Compute SHAP values using DeepExplainer\n",
    "model_transformer.eval()\n",
    "X_train_sample_seq_tensor = torch.tensor(X_train_sample_seq, dtype=torch.float32)\n",
    "explainer_shap_transformer = shap.DeepExplainer(model_transformer, X_train_sample_seq_tensor)\n",
    "shap_values_shap_transformer = explainer_shap_transformer.shap_values(torch.tensor(X_test_sample_seq, dtype=torch.float32), check_additivity=False)\n",
    "\n",
    "# Ensure correct output format\n",
    "if isinstance(shap_values_shap_transformer, list):\n",
    "    shap_values_shap_transformer = shap_values_shap_transformer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize custom SHAP approximator\n",
    "custom_shap_approximator_transformer = SequentialSHAPApproximator(model_transformer, X_train_sample_seq, num_samples=100)\n",
    "\n",
    "# Compute SHAP values using custom approximator\n",
    "shap_values_custom_transformer = custom_shap_approximator_transformer.compute_shap_values(X_test_sample_seq)\n",
    "\n",
    "# Compute SHAP values using custom approximator in batch mode\n",
    "shap_values_custom_transformer_batch = custom_shap_approximator_transformer.batch_approximate_shap_values(X_test_sample_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean absolute SHAP values for the original SHAP values\n",
    "mean_abs_shap_values_transformer = np.mean(np.abs(shap_values_shap_transformer), axis=0).flatten()\n",
    "# Compute mean absolute SHAP values for the custom SHAP values (per sample and batch mode)\n",
    "mean_abs_custom_shap_values_transformer = np.mean(np.abs(shap_values_custom_transformer), axis=0).flatten()\n",
    "mean_abs_custom_shap_values_transformer_batch = np.mean(np.abs(shap_values_custom_transformer_batch), axis=0).flatten()\n",
    "\n",
    "# Normalize SHAP values\n",
    "norm_mean_abs_shap_values_transformer = mean_abs_shap_values_transformer / np.sum(mean_abs_shap_values_transformer)\n",
    "norm_mean_abs_custom_shap_values_transformer = mean_abs_custom_shap_values_transformer / np.sum(mean_abs_custom_shap_values_transformer)\n",
    "norm_mean_abs_custom_shap_values_transformer_batch = mean_abs_custom_shap_values_transformer_batch / np.sum(mean_abs_custom_shap_values_transformer_batch)\n",
    "\n",
    "# Create DataFrame for comparison\n",
    "time_steps = np.arange(len(true_influences))\n",
    "comparison_transformer_df = pd.DataFrame({\n",
    "    'Time Step': time_steps,\n",
    "    'True Influence': true_influences,\n",
    "    'Original SHAP': norm_mean_abs_shap_values_transformer,\n",
    "    'Custom SHAP (Per Sample)': norm_mean_abs_custom_shap_values_transformer,\n",
    "    'Custom SHAP (Batch)': norm_mean_abs_custom_shap_values_transformer_batch\n",
    "})\n",
    "\n",
    "print(comparison_transformer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison over time steps\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(comparison_transformer_df['Time Step'], comparison_transformer_df['True Influence'], marker='o', label='True Influence')\n",
    "plt.plot(comparison_transformer_df['Time Step'], comparison_transformer_df['Original SHAP'], marker='x', label='Original SHAP')\n",
    "plt.plot(comparison_transformer_df['Time Step'], comparison_transformer_df['Custom SHAP (Per Sample)'], marker='s', label='Custom SHAP (Per Sample)')\n",
    "plt.plot(comparison_transformer_df['Time Step'], comparison_transformer_df['Custom SHAP (Batch)'], marker='^', label='Custom SHAP (Batch)')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Normalized Importance')\n",
    "plt.title('Comparison of True Influences and SHAP Values Over Time Steps (Transformer)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with true influences\n",
    "corr_true_original_transformer, _ = pearsonr(true_influences, norm_mean_abs_shap_values_transformer)\n",
    "corr_true_custom_transformer, _ = pearsonr(true_influences, norm_mean_abs_custom_shap_values_transformer)\n",
    "corr_true_custom_transformer_batch, _ = pearsonr(true_influences, norm_mean_abs_custom_shap_values_transformer_batch)\n",
    "\n",
    "print(f'Correlation between True Influences and Original SHAP (Transformer): {corr_true_original_transformer:.4f}')\n",
    "print(f'Correlation between True Influences and Custom SHAP (Per Sample, Transformer): {corr_true_custom_transformer:.4f}')\n",
    "print(f'Correlation between True Influences and Custom SHAP (Batch, Transformer): {corr_true_custom_transformer_batch:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
